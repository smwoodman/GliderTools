{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-order-of-example-code-follows-the-cheat-sheet-for-ease-of-use-and-understanding\" data-toc-modified-id=\"The-order-of-example-code-follows-the-cheat-sheet-for-ease-of-use-and-understanding-0.1\">The order of example code follows the cheat sheet for ease of use and understanding</a></span></li></ul></li><li><span><a href=\"#Import-GliderTools\" data-toc-modified-id=\"Import-GliderTools-1\">Import <code>GliderTools</code></a></span></li><li><span><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-2\">Loading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Working-with-Seaglider-base-station-files\" data-toc-modified-id=\"Working-with-Seaglider-base-station-files-2.1\">Working with Seaglider base station files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-variables\" data-toc-modified-id=\"Load-variables-2.1.1\">Load variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Coordinates-and-automatic-time-fetching\" data-toc-modified-id=\"Coordinates-and-automatic-time-fetching-2.1.1.1\">Coordinates and automatic <em>time</em> fetching</a></span></li><li><span><a href=\"#Merging-data-based-on-time\" data-toc-modified-id=\"Merging-data-based-on-time-2.1.1.2\">Merging data based on time</a></span></li><li><span><a href=\"#Metadata-handling\" data-toc-modified-id=\"Metadata-handling-2.1.1.3\">Metadata handling</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-3\">Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-filtering:-outlier-limits-(IQR-&amp;-STD)\" data-toc-modified-id=\"Global-filtering:-outlier-limits-(IQR-&amp;-STD)-3.1\">Global filtering: outlier limits (IQR &amp; STD)</a></span></li><li><span><a href=\"#Horizontal-filtering:-differential-outliers\" data-toc-modified-id=\"Horizontal-filtering:-differential-outliers-3.2\">Horizontal filtering: differential outliers</a></span></li><li><span><a href=\"#Vertical-smoothing-approaches\" data-toc-modified-id=\"Vertical-smoothing-approaches-3.3\">Vertical smoothing approaches</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Despiking\" data-toc-modified-id=\"Despiking-3.3.0.1\">Despiking</a></span></li></ul></li><li><span><a href=\"#Rolling-window\" data-toc-modified-id=\"Rolling-window-3.3.1\">Rolling window</a></span></li><li><span><a href=\"#Savitzky-Golay\" data-toc-modified-id=\"Savitzky-Golay-3.3.2\">Savitzky-Golay</a></span></li></ul></li><li><span><a href=\"#Cleaning-Wrapper\" data-toc-modified-id=\"Cleaning-Wrapper-3.4\">Cleaning Wrapper</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Insert-data-into-original-dataset\" data-toc-modified-id=\"Insert-data-into-original-dataset-3.4.0.1\">Insert data into original dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Effects-of-smoothing-on-profiles\" data-toc-modified-id=\"Effects-of-smoothing-on-profiles-3.5\">Effects of smoothing on profiles</a></span></li></ul></li><li><span><a href=\"#Deriving-secondary-physical-variables\" data-toc-modified-id=\"Deriving-secondary-physical-variables-4\">Deriving secondary physical variables</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Density\" data-toc-modified-id=\"Density-4.0.1\">Density</a></span></li><li><span><a href=\"#Mixed-Layer-Depth\" data-toc-modified-id=\"Mixed-Layer-Depth-4.0.2\">Mixed Layer Depth</a></span></li></ul></li></ul></li><li><span><a href=\"#Optics\" data-toc-modified-id=\"Optics-5\">Optics</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Backscatter\" data-toc-modified-id=\"Backscatter-5.0.1\">Backscatter</a></span><ul class=\"toc-item\"><li><span><a href=\"#Outlier-bounds-method\" data-toc-modified-id=\"Outlier-bounds-method-5.0.1.1\">Outlier bounds method</a></span></li><li><span><a href=\"#Removing-bad-profiles\" data-toc-modified-id=\"Removing-bad-profiles-5.0.1.2\">Removing bad profiles</a></span></li><li><span><a href=\"#Conversion-from-counts-to-total-backscatter\" data-toc-modified-id=\"Conversion-from-counts-to-total-backscatter-5.0.1.3\">Conversion from counts to total backscatter</a></span></li><li><span><a href=\"#Correcting-for-an-in-situ-dark-count\" data-toc-modified-id=\"Correcting-for-an-in-situ-dark-count-5.0.1.4\">Correcting for an in situ dark count</a></span></li><li><span><a href=\"#Despiking\" data-toc-modified-id=\"Despiking-5.0.1.5\">Despiking</a></span></li><li><span><a href=\"#Adding-the-corrected-variables-to-the-original-dataframe\" data-toc-modified-id=\"Adding-the-corrected-variables-to-the-original-dataframe-5.0.1.6\">Adding the corrected variables to the original dataframe</a></span></li><li><span><a href=\"#Wrapper-function-demonstration\" data-toc-modified-id=\"Wrapper-function-demonstration-5.0.1.7\">Wrapper function demonstration</a></span></li></ul></li><li><span><a href=\"#PAR\" data-toc-modified-id=\"PAR-5.0.2\">PAR</a></span><ul class=\"toc-item\"><li><span><a href=\"#PAR-Scaling\" data-toc-modified-id=\"PAR-Scaling-5.0.2.1\">PAR Scaling</a></span></li><li><span><a href=\"#Correcting-for-an-in-situ-dark-count\" data-toc-modified-id=\"Correcting-for-an-in-situ-dark-count-5.0.2.2\">Correcting for an in situ dark count</a></span></li><li><span><a href=\"#PAR-replacement\" data-toc-modified-id=\"PAR-replacement-5.0.2.3\">PAR replacement</a></span></li><li><span><a href=\"#Wrapper-function-demonstration\" data-toc-modified-id=\"Wrapper-function-demonstration-5.0.2.4\">Wrapper function demonstration</a></span></li></ul></li><li><span><a href=\"#Deriving-additional-variables\" data-toc-modified-id=\"Deriving-additional-variables-5.0.3\">Deriving additional variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Euphotic-Depth-and-Light-attenuation-coefficient\" data-toc-modified-id=\"Euphotic-Depth-and-Light-attenuation-coefficient-5.0.3.1\">Euphotic Depth and Light attenuation coefficient</a></span></li></ul></li><li><span><a href=\"#Fluorescence\" data-toc-modified-id=\"Fluorescence-5.0.4\">Fluorescence</a></span><ul class=\"toc-item\"><li><span><a href=\"#Outlier-bounds-method\" data-toc-modified-id=\"Outlier-bounds-method-5.0.4.1\">Outlier bounds method</a></span></li><li><span><a href=\"#Removing-bad-profiles\" data-toc-modified-id=\"Removing-bad-profiles-5.0.4.2\">Removing bad profiles</a></span></li><li><span><a href=\"#Correcting-for-an-in-situ-dark-count\" data-toc-modified-id=\"Correcting-for-an-in-situ-dark-count-5.0.4.3\">Correcting for an in situ dark count</a></span></li><li><span><a href=\"#Despiking\" data-toc-modified-id=\"Despiking-5.0.4.4\">Despiking</a></span></li><li><span><a href=\"#Quenching-Correction\" data-toc-modified-id=\"Quenching-Correction-5.0.4.5\">Quenching Correction</a></span></li><li><span><a href=\"#Wrapper-function\" data-toc-modified-id=\"Wrapper-function-5.0.4.6\">Wrapper function</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Calibration-with-bottle-samples\" data-toc-modified-id=\"Calibration-with-bottle-samples-6\">Calibration with bottle samples</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Using-depth\" data-toc-modified-id=\"Using-depth-6.0.1\">Using depth</a></span></li><li><span><a href=\"#Using-Density\" data-toc-modified-id=\"Using-Density-6.0.2\">Using Density</a></span></li></ul></li></ul></li><li><span><a href=\"#Gridding-and-interpolation\" data-toc-modified-id=\"Gridding-and-interpolation-7\">Gridding and interpolation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Vertical-gridding\" data-toc-modified-id=\"Vertical-gridding-7.0.1\">Vertical gridding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gridding-with-automatic-bin-sizes\" data-toc-modified-id=\"Gridding-with-automatic-bin-sizes-7.0.1.1\">Gridding with automatic bin sizes</a></span></li><li><span><a href=\"#Gridding-with-manually-defined-bins\" data-toc-modified-id=\"Gridding-with-manually-defined-bins-7.0.1.2\">Gridding with manually defined bins</a></span></li></ul></li><li><span><a href=\"#2D-interpolation-with-objective-mapping-(Kriging)\" data-toc-modified-id=\"2D-interpolation-with-objective-mapping-(Kriging)-7.0.2\">2D interpolation with objective mapping (Kriging)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1:-Semivariance\" data-toc-modified-id=\"Part-1:-Semivariance-7.0.2.1\">Part 1: Semivariance</a></span></li><li><span><a href=\"#2.-Interpolation\" data-toc-modified-id=\"2.-Interpolation-7.0.2.2\">2. Interpolation</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#3D-interactive-plot\" data-toc-modified-id=\"3D-interactive-plot-8\">3D interactive plot</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T09:50:24.856908Z",
     "start_time": "2019-06-10T09:50:24.793463Z"
    }
   },
   "source": [
    "## The order of example code follows the cheat sheet for ease of use and understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T10:11:23.755470Z",
     "start_time": "2019-06-10T10:11:23.750218Z"
    }
   },
   "source": [
    "<img src=\"../docs/img/package_overview.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import `GliderTools`\n",
    "\n",
    "**N.B** to run this notebook, use the environment in the repo https://github.com/GliderToolsCommunity/GliderTools .binder/environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:06.205695Z",
     "start_time": "2019-08-13T14:47:05.268632Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# pylab for more MATLAB like environment and inline displays plots below cells \n",
    "%pylab inline\n",
    "sys.path.append(\"..\")\n",
    "# if gsw Warning shows, manually install gsw if possible - will still work without\n",
    "import glidertools as gt\n",
    "from cmocean import cm as cmo  # we use this for colormaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Seaglider base station files\n",
    "\n",
    "GliderTools supports loading Seaglider files, including `scicon` data (different sampling frequencies).  \n",
    "There is a function that makes it easier to find variable names that you'd like to load: `gt.load.seaglider_show_variables`  \n",
    "\n",
    "This function is demonstrated in the cell below.\n",
    "The function accepts a **list of file names** and can also receive a string with a wildcard placeholder (`*`) and basic regular expressions are also supported. In the example below we use a simple asterisk placeholder for all the files. \n",
    "\n",
    "Note that the function chooses only one file from the passed list or glob string - this file name will be shown. The returned table shows the variable name, dimensions, units and brief comment if it is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:06.505548Z",
     "start_time": "2019-08-13T14:47:06.208113Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = '../tests/data/p5420*.nc'\n",
    "\n",
    "gt.load.seaglider_show_variables(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load variables\n",
    "\n",
    "From the variable listing, one can choose multiple variables to load. Note that one only needs the variable name to load the data. Below, we've created a list of variables that we'll be using for this demo.\n",
    "\n",
    "The `gt.load.seaglider_basestation_netCDFs` function is used to load a list of variables. It requires the filename string or list (as described above) and keys. It may be that these variables are not sampled at the same frequency. In this case, the loading function will load the sampling frequency dimensions separately. The function will try to find a time variable for each sampling frequency/dimension. \n",
    "\n",
    "#### Coordinates and automatic *time* fetching\n",
    "All associated coordinate variables will also be loaded with the data if coordinates are documented. These may included *latitude, longitude, depth* and *time* (naming may vary). If time cannot be found for a dimension, a *time* variable from a different dimension with the same number of observations is used instead. This insures that data can be merged based on the time of sampling. \n",
    "\n",
    "#### Merging data based on time\n",
    "If the `return_merged` is set to *True*, the function will merge the dimensions if the dimension has an associated *time* variable. \n",
    "\n",
    "The function returns a dictionary of `xarray.Datasets` - a Python package that deals with coordinate indexed multi-dimensional arrays. We recommend that you read the documentation (http://xarray.pydata.org/en/stable/) as this package is used throughout *GliderTools*. This allows the original metadata to be copied with the data. The dictionary keys are the names of the dimensions. If `return_merged` is set to *True* an additional entry under the key `merged` will be included.\n",
    "\n",
    "The structure of a dimension output is shown below. Note that the merged data will use the largest dimension as the primary dataset and the other data will be merged onto that time index. Data is linearly interpolated to the nearest time measurement of the primary index, but only by one measurement to ensure transparancy.\n",
    "\n",
    "#### Metadata handling\n",
    "If the keyword arguement `keep_global_attrs=True`, the attributes from the original files (for all that are the same) are passed on to the output *Datasets* from the original netCDF attributes. The variable attributes (units, comments, axis...) are passed on by default, but can also be set to False if not wanted. GliderTools functions will automatically pass on these attributes to function outputs if a `xarray.DataArray` with attributes is given. \n",
    "All functions applied to data will also be recorded under the variable attribute `processing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:19.231878Z",
     "start_time": "2019-08-13T14:47:06.733554Z"
    }
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    'ctd_depth',\n",
    "    'ctd_time',\n",
    "    'ctd_pressure',\n",
    "    'salinity',\n",
    "    'temperature',\n",
    "    'eng_wlbb2flvmt_Chlsig',\n",
    "    'eng_wlbb2flvmt_wl470sig',\n",
    "    'eng_wlbb2flvmt_wl700sig',\n",
    "    'aanderaa4330_dissolved_oxygen',\n",
    "    'eng_qsp_PARuV',\n",
    "]\n",
    "\n",
    "ds_dict = gt.load.seaglider_basestation_netCDFs(filenames, names, return_merged=True, keep_global_attrs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:19.241236Z",
     "start_time": "2019-08-13T14:47:19.234229Z"
    }
   },
   "outputs": [],
   "source": [
    "# the returned data contains the dimensions of the requested variables\n",
    "# a `merged` object is also returned if return_merged=True\n",
    "print(ds_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:19.262608Z",
     "start_time": "2019-08-13T14:47:19.244040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we drop the time variables imported for the PAR variable\n",
    "# we don't need these anymore. You might have to change this \n",
    "# depening on the dataset\n",
    "merged = ds_dict['merged']\n",
    "if 'time' in merged:\n",
    "    merged = merged.drop([\"time\", \"time_dt64\"])\n",
    "\n",
    "\n",
    "# To make it easier and clearer to work with, we rename the \n",
    "# original variables to something that makes more sense. This\n",
    "# is done with the xarray.Dataset.rename({}) function.\n",
    "# We only use the merged dataset as this contains all the \n",
    "# imported dimensions. \n",
    "# NOTE: The renaming has to be specific to the dataset otherwise an error will occur\n",
    "dat = merged.rename({\n",
    "    'salinity': 'salt_raw',\n",
    "    'temperature': 'temp_raw',\n",
    "    'ctd_pressure': 'pressure',\n",
    "    'ctd_depth': 'depth',\n",
    "    'ctd_time_dt64': 'time',\n",
    "    'ctd_time': 'time_raw',\n",
    "    'eng_wlbb2flvmt_wl700sig': 'bb700_raw',\n",
    "    'eng_wlbb2flvmt_wl470sig': 'bb470_raw',\n",
    "    'eng_wlbb2flvmt_Chlsig': 'flr_raw',\n",
    "    'eng_qsp_PARuV': 'par_raw',\n",
    "    'aanderaa4330_dissolved_oxygen': 'oxy_raw',\n",
    "})\n",
    "\n",
    "print(dat)\n",
    "\n",
    "# variable assignment for conveniant access\n",
    "depth = dat.depth\n",
    "dives = dat.dives\n",
    "lats = dat.latitude\n",
    "lons = dat.longitude\n",
    "time = dat.time\n",
    "pres = dat.pressure\n",
    "temp = dat.temp_raw\n",
    "salt = dat.salt_raw\n",
    "par = dat.par_raw\n",
    "bb700 = dat.bb700_raw\n",
    "bb470 = dat.bb470_raw\n",
    "fluor = dat.flr_raw\n",
    "\n",
    "# name coordinates for quicker plotting\n",
    "x = dat.dives\n",
    "y = dat.depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cleaning` module contains several tools that help to remove erroneous data - profiles or points. \n",
    "These filters can be applied *globally* (IQR and standard devation limits), *vertically* (running average filters) or *horizontally* (horizontal filters on gridded data only). \n",
    "\n",
    "There are also two approaches one can use to clean data: 1) filtering out bad points/dives; 2) smoothing data.\n",
    "\n",
    "Below we use **salinity** to demonstrate the different functions available to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:15.670448Z",
     "start_time": "2019-08-13T14:41:14.748444Z"
    }
   },
   "outputs": [],
   "source": [
    "gt.plot(x, y, salt, cmap=cmo.haline, robust=True)\n",
    "title('Original Data')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global filtering: outlier limits (IQR & STD)\n",
    "These functions find upper and lower limits for data outliers using interquartile range and standard deviations of the entire dataset. Multipliers can be set to make the filters more or less strict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:17.720975Z",
     "start_time": "2019-08-13T14:41:15.672533Z"
    }
   },
   "outputs": [],
   "source": [
    "salt_iqr = gt.cleaning.outlier_bounds_iqr(salt, multiplier=1.5)\n",
    "salt_std = gt.cleaning.outlier_bounds_std(salt, multiplier=1.5)\n",
    "\n",
    "# Plotting \n",
    "gt.plot(x, y, salt_iqr, cmap=cmo.haline, robust=True)\n",
    "title('Outlier Bounds IQR Method')\n",
    "\n",
    "gt.plot(x, y, salt_std, cmap=cmo.haline, robust=True)\n",
    "title('Outlier Bounds Stdev Method')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:17.731020Z",
     "start_time": "2019-08-13T14:41:17.723256Z"
    }
   },
   "outputs": [],
   "source": [
    "print(salt_std)\n",
    "print('\\n\\nExample of processing stored as attribute under history\\n' + '-'*41)\n",
    "print(salt_std.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal filtering: differential outliers \n",
    "Erroneous measurements often occur sequentially - i.e. in the vertical. The vertical filtering approaches would thus miss any outliers as rolling windows are often used. It is thus useful to have an approach that compares dives in the horizontal. The `horizontal_diff_outliers` first grids data and then calculates where gradients (rolling mean - measurement) are outliers (same as `outlier_bounds_std`). If a certain fraction of measurements in a dive exceed the threshold, then that dive is deemed a bad dive. The example below shows three dives that have anomalous measurements.  These fall well within the global bounds of acceptable data, but horizontally that are masked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:20.446024Z",
     "start_time": "2019-08-13T14:41:17.732598Z"
    }
   },
   "outputs": [],
   "source": [
    "salt_horz = gt.cleaning.horizontal_diff_outliers(x, y, salt, multiplier=3, depth_threshold=400, mask_frac=0.1)\n",
    "\n",
    "gt.plot(x, y, salt, cmap=cmo.haline)\n",
    "title('Original dataset')\n",
    "show()\n",
    "\n",
    "gt.plot(x, y, salt_horz, cmap=cmo.haline)\n",
    "title('Horizontal Differential Outliers removed')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical smoothing approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despiking\n",
    "This approach was used by Briggs et al. (2010). The idea is to apply a rolling filter to the data (along the time dimension). This forms the baseline. The difference from the original data are spikes. \n",
    "\n",
    "There are two rolling filters that can be applied to the data. The *median* approach is the equivalent of a rolling median. The *minmax* approach first applies a rolling minimum and then rolling maximum to data. This is useful particularly for optics data where spikes are particles in the water column and are not normally distributed. \n",
    "\n",
    "In the case of salinity, the *median* approach is likely best, as \"spikes\" would be positive and negative (Gaussian distribution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:22.320784Z",
     "start_time": "2019-08-13T14:41:20.447888Z"
    }
   },
   "outputs": [],
   "source": [
    "salt_base, salt_spike = gt.cleaning.despike(salt, window_size=5, spike_method='median')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, salt_base, cmap=cmo.haline, ax=ax[0])\n",
    "ax[0].set_title('Despiked using median filter')\n",
    "ax[0].cb.set_label('Salinity baseline')\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "gt.plot(x, y, salt_spike, cmap=cm.RdBu_r, vmin=-6e-3, vmax=6e-3, ax=ax[1])\n",
    "ax[1].cb.set_label('Salinity spikes')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling window\n",
    "\n",
    "The rolling window method simply applies an aggregating function (`mean, median, std, min, max`) to the dataset. \n",
    "Because the above example is equivalent to a rolling median, we show what a rolling `75th percentile` looks like instead. \n",
    "\n",
    "This could be used to create additional filters by users. Note that in this more complex example we create a wrapper function for the percentile so that we can tell the percentile function that we want the 75th percentile and we want to calculate this along the nth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:24.292198Z",
     "start_time": "2019-08-13T14:41:22.322183Z"
    }
   },
   "outputs": [],
   "source": [
    "def seventyfith(x, axis=0):\n",
    "    # wrapper function so we can pass axis and percentile to \n",
    "    # the input function\n",
    "    return np.percentile(x, 75, axis=axis)\n",
    "\n",
    "# other numpy functions also work: np.mean, np.median, np.std\n",
    "salt_roll75 = gt.cleaning.rolling_window(salt, seventyfith, window=5)\n",
    "salt_rollavg = gt.cleaning.rolling_window(salt, mean, window=5)\n",
    "\n",
    "# PLOTTING\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, salt_roll75, cmap=cmo.haline, ax=ax[0])\n",
    "ax[0].set_title('75$^{th}$ for a rolling window with size 5')\n",
    "ax[0].cb.set_label('Salinity baseline')\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "gt.plot(x, y, salt_roll75 - salt, cmap=cm.RdBu_r, vmin=-6e-3, vmax=6e-3, ax=ax[1])\n",
    "ax[1].cb.set_label('Difference from original data')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savitzky-Golay \n",
    "The Savitzky-Golay function fits a low order polynomial to a rolling window of the time series. This has the result of smoothing the data. A larger window with a lower order polynomial with have a smoother fit.\n",
    "\n",
    "We recommend a 2nd order kernel. Here we use first order to show that the difference can be quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:26.079528Z",
     "start_time": "2019-08-13T14:41:24.293620Z"
    }
   },
   "outputs": [],
   "source": [
    "salt_savgol = gt.cleaning.savitzky_golay(salt, window_size=11, order=1)\n",
    "\n",
    "# PLOTTING\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, salt_savgol, cmap=cmo.haline, ax=ax[0])\n",
    "ax[0].set_title('Smoothing the data with Savitzky-Golay')\n",
    "ax[0].cb.set_label('Smoothed salinity')\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "gt.plot(x, y, salt_savgol - salt, cmap=cm.RdBu, vmin=-6e-3, vmax=6e-3, ax=ax[1])\n",
    "ax[1].cb.set_label('Difference from original')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Wrapper\n",
    "\n",
    "Wrapper functions have been designed to make this process more efficient, which is demonstrated below with **temperature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:22.486798Z",
     "start_time": "2019-08-13T14:47:19.265114Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_qc = gt.calc_physics(temp, x, y, \n",
    "                          iqr=1.5, depth_threshold=0,\n",
    "                          spike_window=5, spike_method='median',\n",
    "                          savitzky_golay_window=11, savitzky_golay_order=2)\n",
    "\n",
    "# PLOTTING\n",
    "fig, ax = plt.subplots(3, 1, figsize=[9, 8.5], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, temp, cmap=cmo.thermal, ax=ax[0])\n",
    "gt.plot(x, y, temp_qc, cmap=cmo.thermal, ax=ax[1])\n",
    "gt.plot(x, y, temp_qc - temp, cmap=cm.RdBu_r, vmin=-0.05, vmax=0.05, ax=ax[2])\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "\n",
    "ax[0].cb.set_label('Original Data')\n",
    "ax[1].cb.set_label('Cleaned Data')\n",
    "ax[2].cb.set_label('Difference from Original')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:26.477971Z",
     "start_time": "2019-08-13T14:47:22.488532Z"
    }
   },
   "outputs": [],
   "source": [
    "salt_qc = gt.calc_physics(salt, x, y, \n",
    "                          mask_frac=0.2, iqr=2.5, \n",
    "                          spike_window=5, spike_method='median', \n",
    "                          savitzky_golay_window=11, savitzky_golay_order=2)\n",
    "        \n",
    "# PLOTTING\n",
    "fig, ax = plt.subplots(3, 1, figsize=[9, 8.5], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, salt, cmap=cmo.haline, ax=ax[0])\n",
    "gt.plot(x, y, salt_qc, cmap=cmo.haline, ax=ax[1])\n",
    "gt.plot(x, y, salt_qc - salt, cmap=cm.RdBu_r, vmin=-0.02, vmax=0.02, ax=ax[2])\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "\n",
    "ax[0].cb.set_label('Original Data')\n",
    "ax[1].cb.set_label('Cleaned Data')\n",
    "ax[2].cb.set_label('Difference from Original')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert data into original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:26.483963Z",
     "start_time": "2019-08-13T14:47:26.479824Z"
    }
   },
   "outputs": [],
   "source": [
    "dat['temp_qc'] = temp_qc\n",
    "dat['salt_qc'] = salt_qc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T12:40:35.256711Z",
     "start_time": "2019-06-10T12:40:35.171498Z"
    }
   },
   "source": [
    "## Effects of smoothing on profiles\n",
    "The plots below show the effects of the vertical smoothing filters on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:36.066683Z",
     "start_time": "2019-08-13T14:41:32.977698Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 3, figsize=[10, 5], dpi=90)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "dive_no = 306\n",
    "\n",
    "idx = dat.dives==dive_no\n",
    "colors = rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].plot(salt[idx],         y[idx], c=colors[0], label='Raw', lw=4)\n",
    "    ax[i].plot(salt_base[idx],    y[idx], c=colors[3], label='Despike window = 3')\n",
    "    ax[i].plot(salt_rollavg[idx], y[idx], c=colors[2], label='Rolling Window = 5')\n",
    "    ax[i].plot(salt_savgol[idx],  y[idx], c='k', label='Savitsky-Golay')\n",
    "    \n",
    "ax[2].barh(y[idx], salt_savgol[idx]  - salt[idx], zorder=100, facecolor='k')\n",
    "ax[2].barh(y[idx], salt_base[idx]    - salt[idx], zorder=100, facecolor=colors[3])\n",
    "ax[2].barh(y[idx], salt_rollavg[idx] - salt[idx], zorder=100, facecolor=colors[2])\n",
    "\n",
    "ax[0].legend(loc=4)\n",
    "\n",
    "ymin, ymax= 0, 100\n",
    "ax[0].fill_between([33, 35], [ymin, ymin], [ymax, ymax], facecolor='k', alpha=0.2)\n",
    "ax[0].set_ylim(500, 0)\n",
    "ax[1].set_ylim(ymax, ymin)\n",
    "ax[0].set_ylabel('Depth [m]', labelpad=15)\n",
    "ax[0].set_xlabel('Salinity', labelpad=15)\n",
    "ax[1].set_xlabel('Salinity', labelpad=15)\n",
    "ax[2].set_xlabel('$\\Delta$Salinity', labelpad=15)\n",
    "ax[1].set_title('Profile ' + str(dive_no))\n",
    "\n",
    "ax[2].set_ylim(ymax, ymin)\n",
    "[a.grid(c='0.75', ls='--') for a in ax]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving secondary physical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density\n",
    "GliderTools provides a wrapper to calculate potential density.  \n",
    "This is done by first calculating potential temperature and then calculating absolute salinity.  \n",
    "A reference depth of `0` is used by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:47.621539Z",
     "start_time": "2019-08-13T14:47:46.531524Z"
    }
   },
   "outputs": [],
   "source": [
    "dens0 = gt.physics.potential_density(salt_qc, temp_qc, pres, lats, lons)\n",
    "dat['density'] = dens0\n",
    "gt.plot(dat.dives, dat.depth, dens0, cmap=cmo.dense)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Layer Depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:39.378680Z",
     "start_time": "2019-08-13T14:41:37.191003Z"
    }
   },
   "outputs": [],
   "source": [
    "mld = gt.physics.mixed_layer_depth(dat, \"density\")\n",
    "mld_smoothed = mld.rolling(10, min_periods=3).mean()\n",
    "mld_mask = gt.utils.mask_below_depth(dat, mld)\n",
    "mld_grid = gt.grid_data(dat.dives, dat.depth, mld_mask, verbose=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[9, 3], dpi=100, sharey=True)\n",
    "\n",
    "mld.plot(ax=ax[0])\n",
    "gt.plot(mld_grid, ax=ax[1])\n",
    "\n",
    "[a.set_ylim(100, 0) for a in ax]\n",
    "\n",
    "ax[0].set_ylabel('Depth (m)')\n",
    "[a.set_xlabel('Dives') for a in ax]\n",
    "xticks(rotation=0)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T20:46:17.965409Z",
     "start_time": "2019-07-09T20:46:17.933556Z"
    }
   },
   "source": [
    "### Backscatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:40.387001Z",
     "start_time": "2019-08-13T14:41:39.380357Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = 124\n",
    "xfactor = 1.076 \n",
    "\n",
    "gt.plot(x, y, bb700, cmap=cmo.delta, vmin=60, vmax=200)\n",
    "title('Original Data')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier bounds method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:42.413029Z",
     "start_time": "2019-08-13T14:41:40.388696Z"
    }
   },
   "outputs": [],
   "source": [
    "bb700_iqr = gt.cleaning.outlier_bounds_iqr(bb700, multiplier=3)\n",
    "bb700_std = gt.cleaning.outlier_bounds_std(bb700, multiplier=3)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, bb700_iqr, cmap=cmo.delta, ax=ax[0], vmin=60, vmax=200)\n",
    "gt.plot(x, y, bb700_std, cmap=cmo.delta, ax=ax[1], vmin=60, vmax=200)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "\n",
    "ax[0].set_title('Outlier IQR')\n",
    "ax[1].set_title('Outlier STD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing bad profiles\n",
    "This function masks bad dives based on mean + std x [1] or median + std x [1] at a reference depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:44.364392Z",
     "start_time": "2019-08-13T14:41:42.414418Z"
    }
   },
   "outputs": [],
   "source": [
    "# find_bad_profiles returns boolean mask and dive numbers\n",
    "# we index only the mask\n",
    "bad_profiles = gt.optics.find_bad_profiles(dives, depth, bb700, \n",
    "                                           ref_depth=300, \n",
    "                                           stdev_multiplier=1, \n",
    "                                           method='median')[0]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "# ~ reverses True to False and vice versa - i.e. we mask bad bad profiles\n",
    "gt.plot(x, y, bb700, cmap=cmo.delta, ax=ax[0], vmin=60, vmax=200)\n",
    "gt.plot(x, y, bb700.where(~bad_profiles), cmap=cmo.delta, ax=ax[1], vmin=60, vmax=200)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "\n",
    "ax[0].set_title('All backscatter data')\n",
    "ax[1].set_title('Bad profiles masked')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion from counts to total backscatter \n",
    "\n",
    "The scale and offset function uses the factory calibration dark count and scale factor.\n",
    "\n",
    "The bback total function uses the coefficients from Zhang et al. (2009) to convert the raw counts into total backscatter (m-1), correcting for temperature and salinity. The $\\chi$ factor and $\\theta$ in this example were taken from Sullivan et al. (2013) and Slade & Boss (2015). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:46.389996Z",
     "start_time": "2019-08-13T14:41:44.365771Z"
    }
   },
   "outputs": [],
   "source": [
    "beta = gt.flo_functions.flo_scale_and_offset(bb700.where(~bad_profiles), 49, 3.217e-5)\n",
    "bbp = gt.flo_functions.flo_bback_total(beta, temp_qc, salt_qc, theta, 700, xfactor)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, beta, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(x, y, bbp, cmap=cmo.delta, ax=ax[1], robust=True)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(400, 0) for a in ax]\n",
    "\n",
    "ax[0].set_title('$\\u03B2$')\n",
    "ax[1].set_title('b$_{bp}$ (m$^{-1}$)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting for an in situ dark count\n",
    "Sensor drift from factory calibration requires an additional correction, the calculation of a dark count in situ. This is calculated from the 95th percentile of backscatter measurements between 200 and 400m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:47.578719Z",
     "start_time": "2019-08-13T14:41:46.391456Z"
    }
   },
   "outputs": [],
   "source": [
    "bbp = gt.optics.backscatter_dark_count(bbp, depth)\n",
    "\n",
    "gt.plot(x, y, bbp, cmap=cmo.delta, robust=True)\n",
    "title('b$_{bp}$ (m$^{-1}$)')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despiking\n",
    "Following the methods outlined in Briggs et al. (2011) to both identify spikes in backscatter and remove them from the baseline backscatter signal. The spikes are retained as the data can be used to address specific science questions, but their presence can decrease the accuracy of the fluorescence quenching function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:52.587989Z",
     "start_time": "2019-08-13T14:41:49.727611Z"
    }
   },
   "outputs": [],
   "source": [
    "bbp_horz = gt.cleaning.horizontal_diff_outliers(x, y, bbp, depth_threshold=10, mask_frac=0.05)\n",
    "bbp_baseline, bbp_spikes = gt.cleaning.despike(bbp_horz, 7, spike_method='minmax')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, bbp_baseline, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(x, y, bbp_spikes, ax=ax[1], cmap=cm.Spectral_r, vmin=0, vmax=0.004)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "\n",
    "ax[0].set_title('Despiked b$_{bp}$ (m$^{-1}$)')\n",
    "ax[1].set_title('b$_{bp}$ (m$^{-1}$) spikes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the corrected variables to the original dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:52.596996Z",
     "start_time": "2019-08-13T14:41:52.589749Z"
    }
   },
   "outputs": [],
   "source": [
    "dat['bbp700'] = bbp_baseline\n",
    "dat['bbp700_spikes'] = bbp_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper function demonstration\n",
    "A wrapper function was also designed, which is demonstrated below with the second wavelength (700 nm). The default option is for verbose to be True, which will provide an output of the different processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:53.881602Z",
     "start_time": "2019-08-13T14:41:52.598718Z"
    }
   },
   "outputs": [],
   "source": [
    "bbp_baseline, bbp_spikes = gt.calc_backscatter(\n",
    "    bb700, temp_qc, salt_qc, dives, depth, 700, 49, 3.217e-5, \n",
    "    spike_window=11, spike_method='minmax', iqr=2., profiles_ref_depth=300,\n",
    "    deep_multiplier=1, deep_method='median', verbose=True)\n",
    "\n",
    "dat['bbp700'] = bbp_baseline\n",
    "dat['bbp700_spikes'] = bbp_spikes\n",
    "\n",
    "ax = gt.plot(x, y, dat.bbp700, cmap=cmo.delta),\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:55.101535Z",
     "start_time": "2019-08-13T14:41:53.883989Z"
    }
   },
   "outputs": [],
   "source": [
    "bbp_baseline, bbp_spikes = gt.calc_backscatter(\n",
    "    bb470, temp_qc, salt_qc, dives, depth, 470, 47, 1.569e-5, \n",
    "    spike_window=7, spike_method='minmax', iqr=3, profiles_ref_depth=300,\n",
    "    deep_multiplier=1, deep_method='median', verbose=True)\n",
    "\n",
    "dat['bbp470'] = bbp_baseline\n",
    "dat['bbp470_spikes'] = bbp_spikes\n",
    "\n",
    "gt.plot(x, y, dat.bbp470, cmap=cmo.delta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAR Scaling\n",
    "\n",
    "This function uses the factory calibration to convert from $\\mu$V to $\\mu$E m$^{-2}$ s$^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:57.062728Z",
     "start_time": "2019-08-13T14:41:55.103296Z"
    }
   },
   "outputs": [],
   "source": [
    "par_scaled = gt.optics.par_scaling(par, 6.202e-4, 10.8)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, par, cmap=cmo.solar, ax=ax[0], robust=True)\n",
    "gt.plot(x, y, par_scaled, cmap=cmo.solar, ax=ax[1], robust=True)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(70, 0) for a in ax]\n",
    "\n",
    "ax[0].set_title('PAR ($\\mu$V)')\n",
    "ax[1].set_title('PAR ($\\mu$E m$^{-2}$ m$^{-1}$)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting for an in situ dark count\n",
    "\n",
    "Sensor drift from factory calibration requires an additional correction, the calculation of a dark count in situ. This is calculated from the median of PAR measurements, with additional masking applied for values before 23:01 and outside the 90th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:58.082117Z",
     "start_time": "2019-08-13T14:41:57.064461Z"
    }
   },
   "outputs": [],
   "source": [
    "par_dark = gt.optics.par_dark_count(par_scaled, dives, depth, time)\n",
    "\n",
    "gt.plot(x, y, par_dark, robust=True, cmap=cmo.solar)\n",
    "ylim(70,0)\n",
    "title('PAR ($\\mu$E m$^{-2}$ m$^{-1}$)')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAR replacement\n",
    "\n",
    "This function removes the top 5 metres from each dive profile, and then algebraically recalculates the surface PAR using an exponential equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:58.752379Z",
     "start_time": "2019-08-13T14:41:58.083747Z"
    }
   },
   "outputs": [],
   "source": [
    "par_filled = gt.optics.par_fill_surface(par_dark, dives, depth, max_curve_depth=80)\n",
    "par_filled[par_filled < 0] = 0\n",
    "par_filled = par_filled.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:59.108882Z",
     "start_time": "2019-08-13T14:41:58.755467Z"
    }
   },
   "outputs": [],
   "source": [
    "i = dives == 232\n",
    "\n",
    "fig, ax = subplots(1, 2, figsize=[6,6], dpi=100)\n",
    "\n",
    "ax[0].plot(par_dark[i], depth[i], lw=0.5, marker='o', ms=5)\n",
    "ax[0].plot(par_filled[i], depth[i], lw=0.5, marker='o', ms=3)\n",
    "ax[1].plot(par_filled[i] - par_dark[i], depth[i], lw=0, marker='o')\n",
    "\n",
    "ax[0].set_ylim(80,0)\n",
    "ax[0].set_ylabel('Depth (m)')\n",
    "ax[0].set_xlabel('PAR ($\\mu$E m$^{-2}$ m$^{-1}$)')\n",
    "\n",
    "ax[1].set_ylim(80,0)\n",
    "ax[1].set_xlim(-350,350)\n",
    "ax[1].set_yticklabels('')\n",
    "ax[1].set_xlabel('Difference between profiles')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:41:59.981184Z",
     "start_time": "2019-08-13T14:41:59.110859Z"
    }
   },
   "outputs": [],
   "source": [
    "gt.plot(x, y, par_filled, robust=True, cmap=cmo.solar)\n",
    "xlim(200,340)\n",
    "ylim(100,0)\n",
    "title('PAR ($\\mu$E m$^{-2}$ m$^{-1}$)')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper function demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:01.330935Z",
     "start_time": "2019-08-13T14:41:59.997643Z"
    }
   },
   "outputs": [],
   "source": [
    "par_qc = gt.calc_par(par, dives, depth, time, \n",
    "                     6.202e-4, 10.8, \n",
    "                     curve_max_depth=80, \n",
    "                     verbose=True).fillna(0)\n",
    "\n",
    "gt.plot(x, y, par_qc, robust=True, cmap=cmo.solar)\n",
    "ylim(80, 0)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving additional variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euphotic Depth and Light attenuation coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:04.873858Z",
     "start_time": "2019-08-13T14:42:02.023332Z"
    }
   },
   "outputs": [],
   "source": [
    "euphotic_depth, kd = gt.optics.photic_depth(par_filled, dives, depth, return_mask=False, ref_percentage=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:05.069184Z",
     "start_time": "2019-08-13T14:42:04.878093Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 1, figsize=[6,4], dpi=100)\n",
    "p1 = plot(euphotic_depth.index, euphotic_depth, label='Euphotic Depth')\n",
    "ylim(120,0)\n",
    "ylabel('Euphotic Depth (m)')\n",
    "xlabel('Dives')\n",
    "ax2 = ax.twinx()\n",
    "p2 = plot(kd.index, kd, color='orange', lw=0, marker='o', ms=2, label='K$_d$')\n",
    "ylabel('K$_d$', rotation=270, labelpad=20)\n",
    "\n",
    "lns = p1+p2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax2.legend(lns, labs, loc=3, numpoints=1)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluorescence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quenching Correcting Method as outlined in Thomalla et al. (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:06.077734Z",
     "start_time": "2019-08-13T14:42:05.076229Z"
    }
   },
   "outputs": [],
   "source": [
    "gt.plot(x, y, fluor, cmap=cmo.delta, robust=True)\n",
    "title('Original Data')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier bounds method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:07.104537Z",
     "start_time": "2019-08-13T14:42:06.080060Z"
    }
   },
   "outputs": [],
   "source": [
    "flr_iqr = gt.cleaning.outlier_bounds_iqr(fluor, multiplier=3)\n",
    "\n",
    "gt.plot(x, y, flr_iqr, cmap=cmo.delta, robust=True)\n",
    "title('Outlier Bounds IQR Method')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing bad profiles\n",
    "\n",
    "This function masks bad dives based on mean + std x [3] or median + std x [3] at a reference depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:09.000149Z",
     "start_time": "2019-08-13T14:42:07.106581Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_profiles = gt.optics.find_bad_profiles(dives, depth, flr_iqr, \n",
    "                                           ref_depth=300, \n",
    "                                           stdev_multiplier=4, \n",
    "                                           method='mean')\n",
    "flr_goodprof = flr_iqr.where(~bad_profiles[0])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, flr_iqr, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(x, y, flr_goodprof, cmap=cmo.delta, ax=ax[1], robust=True)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(300, 0) for a in ax]\n",
    "\n",
    "ax[0].set_title('Bad Profiles Included')\n",
    "ax[1].set_title('Bad Profiles Discarded')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting for an in situ dark count\n",
    "\n",
    "Sensor drift from factory calibration requires an additional correction, the calculation of a dark count in situ. This is calculated from the 95th percentile of fluorescence measurements between 300 and 400m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:10.013914Z",
     "start_time": "2019-08-13T14:42:09.001810Z"
    }
   },
   "outputs": [],
   "source": [
    "flr_dark = gt.optics.fluorescence_dark_count(flr_iqr, dat.depth)\n",
    "\n",
    "gt.plot(x, y, flr_dark, cmap=cmo.delta, robust=True)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despiking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:12.079876Z",
     "start_time": "2019-08-13T14:42:10.016485Z"
    }
   },
   "outputs": [],
   "source": [
    "flr_base, flr_spikes = gt.cleaning.despike(flr_dark, 11, spike_method='median')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, flr_base, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(x, y, flr_spikes, cmap=cm.RdBu_r, ax=ax[1], vmin=-5, vmax=5)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(300, 0) for a in ax]\n",
    "\n",
    "ax[0].set_title('Despiked Fluorescence')\n",
    "ax[1].set_title('Fluorescence spikes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quenching Correction\n",
    "\n",
    "This function uses the method outlined in Thomalla et al. (2017), briefly it calculates the quenching depth and performs the quenching correction based on the fluorescence to backscatter ratio. The quenching depth is calculated based upon the different between night and daytime fluorescence.\n",
    "\n",
    "The default setting is for the preceding night to be used to correct the following day's quenching (`night_day_group=True`). This can be changed so that the following night is used to correct the preceding day. The quenching depth is then found from the difference between the night and daytime fluorescence, using the steepest gradient of the {5 minimum differences and the points the difference changes sign (+ve/-ve)}.\n",
    "\n",
    "The function gets the backscatter/fluorescence ratio between from the quenching depth to the surface, and then calculates a mean nighttime ratio for each night. The quenching ratio is calculated from the nighttime ratio and the daytime ratio, which is then applied to fluorescence to correct for quenching. If the corrected value is less than raw, then the function will return the original raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:15.699550Z",
     "start_time": "2019-08-13T14:42:12.081853Z"
    }
   },
   "outputs": [],
   "source": [
    "flr_qc, quench_layer = gt.optics.quenching_correction(\n",
    "    flr_base, dat.bbp470, dives, depth, time, lats, lons,\n",
    "    sunrise_sunset_offset=1, night_day_group=True)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(x, y, flr_qc, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(x, y, quench_layer, cmap=cm.RdBu_r, ax=ax[1], vmin=-.5, vmax=2)\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(100, 0) for a in ax]\n",
    "\n",
    "ax[0].set_title('Quenching Corrected Fluorescence')\n",
    "ax[1].set_title('Quenching Layer')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:42:23.507749Z",
     "start_time": "2019-08-13T14:42:15.701384Z"
    }
   },
   "outputs": [],
   "source": [
    "flr_qnch, flr, qnch_layer, [fig1, fig2] = gt.calc_fluorescence(\n",
    "    fluor, dat.bbp700, dives, depth, time, lats, lons, 53, 0.0121,\n",
    "    profiles_ref_depth=300, deep_method='mean', deep_multiplier=1,\n",
    "    spike_window=11, spike_method='median', return_figure=True, \n",
    "    night_day_group=False, sunrise_sunset_offset=2, verbose=True)\n",
    "\n",
    "dat['flr_qc'] = flr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration with bottle samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottle calibration can also be done using the `calibration` module. \n",
    "\n",
    "The bottle file needs to be in a specific format with dates (`datetime64` format), depth and the variable values. This can be imported with any method available. I recommend `pandas.read_csv` as shown in the example below. Note that latitude and longitude are not taken into account, thus the user needs to make sure that the CTD cast was in the correct location (and time, but this will be used to match the glider). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:26.538900Z",
     "start_time": "2019-08-13T14:47:26.485807Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = '/Users/luke/Work/Publications/2019_Gregor_Front_glider/figures/SOSCEX 3 PS1.csv'\n",
    "cal = pd.read_csv(fname, parse_dates=['datetime'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `calibration.bottle_matchup` function returns an array that matches the size of the ungridded glider data. \n",
    "The matching is done based on depth and time from both the glider and the CTD. The function will show how many samples have been matched and the smallest time difference between a CTD rosette cast and a dive (any time on the dive). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:47:29.531514Z",
     "start_time": "2019-08-13T14:47:28.757249Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2 \n",
    "\n",
    "dat['bottle_sal'] = gt.calibration.bottle_matchup(\n",
    "    dat.dives, dat.depth, dat.time, \n",
    "    cal.depth, cal.datetime, cal.sal)\n",
    "\n",
    "model = gt.calibration.robust_linear_fit(dat.salt_qc, dat.bottle_sal, fit_intercept=True, epsilon=1.5)\n",
    "dat['salinity_qc'] = model.predict(dat.salt_qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:48:39.298630Z",
     "start_time": "2019-08-13T14:48:38.798810Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "dat['bottle_sal'] = gt.calibration.bottle_matchup(\n",
    "    dat.dives, dat.density, dat.time, \n",
    "    cal.density, cal.datetime, cal.sal)\n",
    "\n",
    "model = gt.calibration.robust_linear_fit(dat.salt_qc, dat.bottle_sal, fit_intercept=True, epsilon=1.5)\n",
    "dat['salinity_qc'] = model.predict(dat.salt_qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridding and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical gridding\n",
    "It is often more convenient and computationally efficient to work with data that has been gridded to a standard vertical grid (i.e. depths have been binned). \n",
    "GliderTools offers very easy to use and efficient tools to grid data once all the processing has been completed. \n",
    "\n",
    "The first task is to select the bin size of the data that will be gridded. \n",
    "GliderTools automatically selects bin sizes according to the sampling frequency of the dataset for every 50m.\n",
    "This is shown in the figure below, where the 2D histogram shows the sampling frequency (by depth) and the line shows the automatically selected bin size rounded up to the nearest 0.5m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:58:06.508870Z",
     "start_time": "2019-08-12T14:58:05.985247Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = gt.plot.bin_size(dat.depth, cmap=mpl.cm.Blues)\n",
    "ax.set_xlim(0, 6)\n",
    "line = ax.get_children()[1]\n",
    "line.set_linewidth(6)\n",
    "line.set_color('orange')\n",
    "\n",
    "legend = ax.get_children()[-2]\n",
    "legend.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridding with automatic bin sizes\n",
    "\n",
    "Gridding the data then becomes easy with automatic binning. But note that the x-coordinate has the be semi-discrete, e.g. dives number or dive time stamp average. You'll see that the gridding function also returns the mean bin size and then the average sampling frequency.\n",
    "\n",
    "The function can return either an xr.DataArray or a pd.DataFrame. The DataArray is the default as metadata can be stored in these files (including coordinate information).\n",
    "\n",
    "Gridded data can be passed to the plot function without x- and y-coordinates, as these are contained in the gridded data. \n",
    "\n",
    "In fact, data is silently passed through the gridding function when x- and y-coordinates are included in the `gt.plot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:58:21.297847Z",
     "start_time": "2019-08-12T14:58:19.467089Z"
    }
   },
   "outputs": [],
   "source": [
    "flr_gridded = gt.grid_data(dives, depth, flr)\n",
    "\n",
    "ax = gt.plot(flr_gridded, cmap=cmo.delta)\n",
    "ax.set_ylim(200, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridding with manually defined bins\n",
    "\n",
    "There is also the option to manuualy define your bins if you'd prefer. \n",
    "A custom bin array needs to be created.  \n",
    "Use `np.arange` to create sections of the bins and combine them with `np.r_` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T14:58:31.526573Z",
     "start_time": "2019-08-12T14:58:28.973662Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_bin = np.r_[\n",
    "    np.arange(0, 100, 0.5),\n",
    "    np.arange(100, 400, 1.0),\n",
    "    np.arange(400, 1000, 2.0)]\n",
    "\n",
    "flr_gridded = gt.grid_data(x, y, flr, bins=custom_bin)\n",
    "\n",
    "# The plot below is the standard plotting procedure for an xarray.DataArray\n",
    "gt.plot(flr_gridded, cmap=cmo.delta)\n",
    "ylim(200, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D interpolation with objective mapping (Kriging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users may want to interpolate data horizontally when working with finescale gradients. \n",
    "Several studies have used the `objmap` MATLAB function that uses objective mapping (a.k.a. Kriging). \n",
    "Kriging is an advanced form of inverse distance weighted interpolation, where points influence the interpolation based on the distance from an interpolation point, where the influence falls off with a Gauassian function. \n",
    "This is an expensive function when the dataset is large (due to a matrix inverse operation). \n",
    "The computational cost is reduced by breaking the problem into smaller pieces using a quadtree that iteratively breaks data into smaller problems. \n",
    "\n",
    "GliderTools provides a Python implementation of the MATLAB function. We have added parallel capability to speed the processing up, but this operation is still costly and could take several hours if an entire section is interpolated. We thus recommend that smaller sections are interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T10:09:32.024475Z",
     "start_time": "2019-08-13T10:09:31.973851Z"
    }
   },
   "outputs": [],
   "source": [
    "# first we select a subset of data (50k points)\n",
    "subs = dat.isel(merged=slice(0, 50000))\n",
    "\n",
    "# we then get time values - this makes creating the interpolation grid easier\n",
    "var = subs.flr_qc\n",
    "time = subs.time.values\n",
    "depth = subs.depth\n",
    "dives = subs.dives\n",
    "dist = np.r_[0, gt.utils.distance(subs.longitude, subs.latitude).cumsum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Semivariance\n",
    "Interpolating any variable requires some knowlege about the spatial autocorrelation of that variable. A semivariogram allows one to get this information from the data. The basic idea of a semivariogram is to assess the similarity between data at different lengthscales (lags), where a low semivariance shows coherence and a large semivariance shows a mismatch. This information is required to interpolate data with sensible estimates and error estimates. \n",
    "\n",
    "GliderTools offers a derivation of a variogram tool (`gt.mapping.variogram`) that makes the process of finding these parameters a little easier, though there is a fair deal of subjectivity, depending on the scale of the question at hand, and tinkering are required to make a sensible interpolation. \n",
    "\n",
    "##### 1.1. Choosing a subset of the data for semivariance estimation\n",
    "The variogram function selects a number of dives (number depends on max_points) and performs the analysis on the subset of dives rathern than selecting random points. We thus recommend that a subset of the data is used to perform the analysis. In the example below, we take a subset of the data that as particularly high variability that we are interested in preserving. This subset is < 250m depth and limited to the first 20 dives. This should be tailored to the variable that you're interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T10:09:33.669993Z",
     "start_time": "2019-08-13T10:09:33.260807Z"
    }
   },
   "outputs": [],
   "source": [
    "m = (depth<150) & (dives > 30) & (dives < 46)\n",
    "ax = gt.plot(dives, depth, var)\n",
    "ax.plot(dives[m], depth[m], '-m', ms=3, alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Initial estimate of semivariance\n",
    "We can now find an initial estimate of the semivariance. This initial estimate will not scale the x/y coordinates for anisotropy (different scales of variability). The variogram function also accepts a boolean mask as an keyword argument. This will reduce the input data to the subset of data that you've chosen. \n",
    "\n",
    "The example below shows this initial estimate. We're looking for an estimate where the Gaussian model fits the semi-variance as well as possible, given that the variance paramters are acceptable. These variance parameters are: *sill, nugget, x and y length-scales*. The function automatically adjusts the range to be one and scales the x and y parameters accordingly. \n",
    "\n",
    "The variogram function can take time (datetime64), but we use distance (in metres) to demonstrate the the anisotropic scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T10:09:35.809030Z",
     "start_time": "2019-08-13T10:09:34.877304Z"
    }
   },
   "outputs": [],
   "source": [
    "vargram = gt.mapping.variogram(var, dist, depth, dives, mask=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above shows that x and y are scaled, but the Gaussian model does not fit the semivariance very well. The range is 1, because it is scaled accordingly. The sill and nugget are very similar - this is not a good result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Finding the correct x and y length scales (anisotropy)\n",
    "\n",
    "We can now scale the data with the xy_ratio. The ratio represents the scaling of x/y. For example, if x and y are both in metres (as in this case), we need to set a small xy_ratio as x has a much longer lengthscale. With some trial and error we choose a ratio of 0.0005, which fits the semivariogram relatively well and has a reasonably low *y* scaling estimate. \n",
    "\n",
    "You'll see that the Gaussian model does not fit the semivariance exactly - this is OK. The important thing is that the first plateau matches the sill. \n",
    "\n",
    "We can now use these values for interpolating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T10:22:10.390520Z",
     "start_time": "2019-08-13T10:22:09.246151Z"
    }
   },
   "outputs": [],
   "source": [
    "vargram = gt.mapping.variogram(var, dist, depth, dives, mask=m, xy_ratio=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Interpolation \n",
    "##### 2.1 Preparing the interpolation grid\n",
    "To perform the interpolation we first need to create the grid onto which data will be interpolated. \n",
    "In the example below we use distance from the origin as the x-coordinate. \n",
    "Time can also be used and has to be in a `np.datetime64` format - we show a commented example of this. \n",
    "The y-coordinate is depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T11:41:01.041662Z",
     "start_time": "2019-08-13T11:41:00.995041Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating the x- and y-interpolation coordinates\n",
    "# and a 1m vertical grid and a horizontal grid with 500 points\n",
    "xi = np.linspace(dist.min(), dist.max(), 500)\n",
    "yi = np.arange(0, depth[var.notnull()].max(), 1, dtype=float)\n",
    "\n",
    "# time can also be used. This is a commented example of how to create \n",
    "# a time grid for interpolation. \n",
    "# xi = np.arange(time.min(), time.max(), 30, dtype='datetime64[m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Interpolation with the semivariance parameters\n",
    "The interpolation has a number of parameters that can be changed or adapted to the dataset at hand.   \n",
    "The commented inputs below describe these inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T11:58:53.599435Z",
     "start_time": "2019-08-13T11:58:30.599501Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "interpolated = gt.mapping.interp_obj(\n",
    "    dist, depth, var, xi, yi, \n",
    "    \n",
    "    # Kriging interoplation arguments\n",
    "    partial_sill=1.1e4,  # taken from the semivariogram (sill - nugget)\n",
    "    nugget=3e3,  # taken from the semivariogram\n",
    "    lenscale_x=98942,  # in hours if x and xi are in datetime64\n",
    "    lenscale_y=50,  # the vertical gridding influence\n",
    "    detrend=True,  # if True use linear regression (z - z_hat), if False use average (z - z_mean)\n",
    "    \n",
    "    # Quadtree arguments\n",
    "    max_points_per_quad=65,  # an optimsation setting ~100 is good\n",
    "    min_points_per_quad=8,  # if neighbours have < N points, look at their neighbours\n",
    "    \n",
    "    # Parallel calculation inputs.\n",
    "    n_cpus=3,  # the number of CPU's to use for the calculation - default is n-1\n",
    "    parallel_chunk_size=512,  # when the dataset is very large, memory can become an issue\n",
    "                              # this prevents large buildup of parallel results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T12:05:51.926197Z",
     "start_time": "2019-08-13T12:05:49.139551Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=[9, 9], sharex=True, dpi=90)\n",
    "\n",
    "error_mask = (interpolated.variance / interpolated.nugget) < 1.05\n",
    "interp_robust = interpolated.z.where(error_mask)\n",
    "\n",
    "props = dict(vmin=0, vmax=300, cmap=cmo.delta)\n",
    "gt.plot.scatter(dist, depth, var, ax=ax[0], **props)\n",
    "gt.plot.pcolormesh(interp_robust, ax=ax[1], **props)\n",
    "gt.plot.pcolormesh(interpolated.variance, ax=ax[2], vmin=interpolated.nugget, vmax=interpolated.nugget*1.08)\n",
    "\n",
    "ax[2].plot(dist, depth, 'w-', zorder=40, alpha=0.8, lw=0.4)\n",
    "\n",
    "[a.set_ylim(400, 0) for a in ax]\n",
    "[a.set_xlabel('  ') for a in ax]\n",
    "\n",
    "ax[0].get_children()[0].set_sizes([20])\n",
    "ax[0].set_title('Uninterpolated data')\n",
    "ax[1].set_title('Interpolated data')\n",
    "ax[2].set_title('Interpolation variance with dives shown in white')\n",
    "ax[2].set_xlabel('Distance (m)')\n",
    "\n",
    "tks = xticks(rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D interactive plot\n",
    "\n",
    "This is purely for investigative purposes, but provides a good way to interact with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:58:55.840231Z",
     "start_time": "2019-08-13T14:58:52.153180Z"
    }
   },
   "outputs": [],
   "source": [
    "plotly_figure = gt.plot.section3D(\n",
    "    dat.dives, dat.depth, dat.longitude, dat.latitude, dat.salt_qc, \n",
    "    zmin=-500, vmax=.999, vmin=.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "136px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "260px",
    "left": "1078.99px",
    "right": "20px",
    "top": "-14.0069px",
    "width": "313px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
